\babel@toc {ngerman}{}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Die zwei Klassen wären hier \glqq grün'' und \glqq blau''. Die Linie stellt die Klassengrenze dar, die die zwei Klassen unterscheidet. Es sind außerdem einige Ausreißer in den Daten vorhanden. \relax }}{4}{figure.caption.2}%
\contentsline {figure}{\numberline {2}{\ignorespaces Die Kurve stellt hier keine Grenze, sondern die Funktion, die die Werte approximiert, dar. Die Punkte repräsentieren die Eingabedaten, wobei auch hier einige Ausreißer erkennbar sind.\relax }}{5}{figure.caption.3}%
\contentsline {figure}{\numberline {3}{\ignorespaces Klassengrenzen mit und ohne Overfitting\relax }}{7}{figure.caption.4}%
\contentsline {figure}{\numberline {4}{\ignorespaces Neuron \newline Quelle: simple.wikipedia.org/wiki/File:Neuron.svg\newline Copyright: CC Attribution-Share Alike von Nutzer Dhp1080,\newline bearbeitet}}{10}{figure.caption.5}%
\contentsline {figure}{\numberline {5}{\ignorespaces Schematische Darstellung eines einfachen neuronalen Netzes\relax }}{11}{figure.caption.6}%
\contentsline {figure}{\numberline {6}{\ignorespaces Darstellung der Sigmoid-Funktion $\sigma (x)=\frac {e^x}{e^x+1}$ im Intervall $[-5, 5]$\relax }}{12}{figure.caption.7}%
\contentsline {figure}{\numberline {7}{\ignorespaces Formel zur Berechnung eines Ausgabevektors aus einem Eingabevektor durch eine Schicht von Neuronen. \relax }}{14}{figure.caption.8}%
\contentsline {figure}{\numberline {8}{\ignorespaces Die Gleichung für den durchschnittlichen quadratischen Fehler\relax }}{15}{figure.caption.9}%
\contentsline {figure}{\numberline {9}{\ignorespaces Die Gleichung für den durchschnittlichen absoluten Fehler\relax }}{16}{figure.caption.10}%
\contentsline {figure}{\numberline {10}{\ignorespaces Der Graph der Kreuzentropie-Fehlerfunktion, wenn das tatsächliche Label 1 ist\relax }}{16}{figure.caption.11}%
\contentsline {figure}{\numberline {11}{\ignorespaces Die Gleichung für den Kreuzentropiefehler\relax }}{17}{figure.caption.12}%
\contentsline {figure}{\numberline {12}{\ignorespaces Die Gleichung für den durchschnittlichen absoluten Fehler\relax }}{17}{figure.caption.13}%
\contentsline {figure}{\numberline {13}{\ignorespaces Die Gleichung für den Gradienten der Fehlerfunktion\relax }}{18}{figure.caption.14}%
\contentsline {figure}{\numberline {14}{\ignorespaces Die Gleichung für die Anpassung eines einzelnen Parameters\relax }}{18}{figure.caption.15}%
\contentsline {figure}{\numberline {15}{\ignorespaces $\eta $ ist hier zu groß gewählt, sodass das Minimum stets übersprungen wird.\relax }}{19}{figure.caption.16}%
\contentsline {figure}{\numberline {16}{\ignorespaces Eine Verbildlichung der Vorgänge in einem convolutional Layer\newline Aus einer Animation von\newline https://github.com/vdumoulin/conv\_arithmetic/blob/master/README.md Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning (BibTeX)}}{20}{figure.caption.17}%
\contentsline {figure}{\numberline {17}{\ignorespaces Erkennt obere horizontale Kanten\relax }}{20}{figure.caption.18}%
\contentsline {figure}{\numberline {18}{\ignorespaces Erkennt linke vertikale Kanten\relax }}{20}{figure.caption.18}%
\contentsline {figure}{\numberline {19}{\ignorespaces Erkennt untere horizontale Kanten\relax }}{20}{figure.caption.18}%
\contentsline {figure}{\numberline {20}{\ignorespaces Erkennt rechte vertikale Kanten\relax }}{20}{figure.caption.18}%
\contentsline {figure}{\numberline {21}{\ignorespaces Das Beispielbild aus dem Mnist Datensatz\relax }}{21}{figure.caption.19}%
\contentsline {figure}{\numberline {22}{\ignorespaces Die jeweils oben stehenden Filter wurden auf das Beispielbild angewandt.\relax }}{21}{figure.caption.20}%
\contentsline {figure}{\numberline {23}{\ignorespaces Beispiele für low-, mid- und high-level Features in Convolutional Neural Nets\newline Quelle: https://tvirdi.github.io/2017-10-29/cnn/}}{22}{figure.caption.21}%
\contentsline {figure}{\numberline {24}{\ignorespaces Max Pooling mit $2\times 2$ großen Submatrizen\newline Quelle: https://computersciencewiki.org/index.php/Max-pooling\_/\_Pooling CC BY NC SA Lizenz}}{23}{figure.caption.22}%
\contentsline {figure}{\numberline {25}{\ignorespaces Average-Pooling mit $2\times 2$ großen Submatrizen\newline Aus: Dominguez-Morales, Juan Pedro. (2018). Neuromorphic audio processing through real-time embedded spiking neural networks. Abbildung 33}}{23}{figure.caption.23}%
\contentsline {figure}{\numberline {26}{\ignorespaces Gegenüberstellung von Max- und Average-Pooling\relax }}{24}{figure.caption.24}%
\contentsline {figure}{\numberline {27}{\ignorespaces Der Code zum Laden des MNIST Datensatzes\relax }}{26}{figure.caption.25}%
\contentsline {figure}{\numberline {28}{\ignorespaces Code, um ein einfaches Netz in Pytorch zu definieren\relax }}{28}{figure.caption.26}%
\contentsline {figure}{\numberline {29}{\ignorespaces Code, um das Netz auf einem Datensatz zu trainieren\relax }}{30}{figure.caption.27}%
\contentsline {figure}{\numberline {30}{\ignorespaces Der Code, um das in diesem Projekt genutzte Klassifizierungsnetz zu definieren.\relax }}{33}{figure.caption.28}%
\contentsline {figure}{\numberline {31}{\ignorespaces Der Graph der ReLu Aktivierungsfunktion\relax }}{34}{figure.caption.29}%
\contentsline {figure}{\numberline {32}{\ignorespaces Ein Plot der Trefferquote, aufgetragen gegen die Trainingszeit\relax }}{34}{figure.caption.30}%
\contentsline {figure}{\numberline {33}{\ignorespaces Ein Plot des Kreuzentropiefehlers aufgetragen gegen die Trainingszeit\relax }}{35}{figure.caption.31}%
